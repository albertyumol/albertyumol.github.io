---
title: "From Paint to PhotoShop real quick: Generating photo-realistic images using semantic image synthesis"
date: 2019-11-03
tags: [image processing, artificial intelligence, deep learning]
header:
  image: "/images/rally/title.png"
  teaser: "/images/animation/time_series.gif"
excerpt: "Style transfer and realistic image rendering using Neural Networks."
mathjax: true

---
<div id="fb-root"></div>
<script async defer src="https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v3.2"></script>

Have you heard the latest buzz from NVDIA? Their engineers where able to create an algorithm that converts semantic images, the ones that you doodle in MS Paint as a toddler, to very realistic images.

Just look at this animation:

{:refdef: style="text-align: center;"}
<img src="{{ site.url }}{{ site.baseurl }}/images/ip/gaugan.gif" alt="Gaugan." class="center">
{: refdef}

I was not born with artistically coordinated hands. I can’t even draw passable stick figures. My right hand sketches are practically as bad as my left. Having a physics background only worsened this ‘skill’ as I was trained to assume and over simplify 3D moving objects as 1D dots and particles connected with hasty crooked arrows. I find art fun but I guess it’s not mutual.

Now, with the advent of deep learning and machine learning in image processing, we can all be the Van Goghs and Picassos that we dreamed of.

To start, here is a [link](https://arxiv.org/abs/1903.07291) of their paper if you don't want any spoilers.

This paper is state-of-the-art in image segmentation and style transfer as of writing so brace yourselves with some maths and ramp up your *machine-learning curves*.

**Image Processing Basics**

There are two main things that you can do with image processing: *classification* and *generation*. Image classification involves various algorithms that categorizes an existing image into a particular class.

Like this Meme:

{:refdef: style="text-align: center;"}
<img src="{{ site.url }}{{ site.baseurl }}/images/ip/meme.jpg" alt="meme" class="center">
{: refdef}

Visually an image classifier algorithm looks like this:

{:refdef: style="text-align: center;"}
<img src="{{ site.url }}{{ site.baseurl }}/images/ip/image_classifier.jpg" alt="meme" class="center">
{: refdef}
[source](https://adamdking.com/blog/gaugan/)

Basically, you have an image with **3 channels** (red, green and blue) and height-width dimension. You apply image decomposition techniques and output a value that you use to classify an image (cat or dog based from the meme above).

On the other hand, image generation is when you create new images from certain inputs.

{:refdef: style="text-align: center;"}
<img src="{{ site.url }}{{ site.baseurl }}/images/ip/generator.jpg" alt="meme" class="center">
{: refdef}
[source](https://adamdking.com/blog/gaugan/)

This time around you make use of that little information to reconstruct an image or generate an entirely new image.

The main imaging technique to achieve both these processes is called convoltion. In simplest terms, convolution is just multiplication in frequency space. Read my [post](https://albertyumol.wixsite.com/bash/activity-4) on Fourier Transforms, Convolution, and Image Formation.

Another term for image classifiers are *discriminators*. A convolution operation reduces the dimension of features based from a pixel window:

{:refdef: style="text-align: center;"}
<img src="{{ site.url }}{{ site.baseurl }}/images/ip/convolution.gif" alt="meme" class="center">
{: refdef}

Image generation works with the same principle but done in reverse.

{:refdef: style="text-align: center;"}
<img src="{{ site.url }}{{ site.baseurl }}/images/ip/convolution_transposed.gif" alt="meme" class="center">
{: refdef}

As a baseline to measure the performance, an image classifier can be cross-validated with a ground truth. The problem is the baseline for testing the performance of an image generator. This problem is addressed by a type of network algorithm called *Generative Adversarial Network* (GAN).

Adversarial because you will train to models that will compete with each other. The generator produces fake images that will be fed as one of the inputs to the discriminator. The discriminator will then try to identify whether the input image is real or fake.

To train the models and make it adaptively learn, we use a loss function given by:

$$
\begin{equation}
\mathcal{L}_\text{GAN}(D, G) = \underbrace{E_{\vec{x} \sim p_\text{data}}[\log D(\vec{x})]}_{\text{accuracy on real images}} + \underbrace{E_{\vec{z} \sim \mathcal{N}}[\log (1 - D(G(\vec{z}))]}_{\text{accuracy on fakes}}
\end{equation}
$$








Want to collaborate? Message me in [LinkedIn](https://ph.linkedin.com/in/albertyumol).



References:

[1] T. Park, etal. [Semantic Image Synthesis with Spatially Adaptive Normalization.](https://arxiv.org/abs/1903.07291)

[2] A. King. [Photos from Crude Sketches: NVIDIA's GauGAN Explained Visually.](https://adamdking.com/blog/gaugan/)





<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-6410209740119334",
    enable_page_level_ads: true
  });
</script>

<div class="fb-comments" data-href="https://albertyumol.github.io/" data-numposts="5"></div>
